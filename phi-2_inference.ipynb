{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig)\n",
    "import threading\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_identifier = \"microsoft/phi-2\"\n",
    "enable_4bit = True\n",
    "compute_dtype_bnb = \"float16\"\n",
    "quant_type_bnb = \"nf4\"\n",
    "double_quant_flag = False\n",
    "device_assignment = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_computation = getattr(torch, compute_dtype_bnb)\n",
    "\n",
    "# BitsAndBytes configuration for model quantization\n",
    "bnb_setup = BitsAndBytesConfig(load_in_4bit=enable_4bit,\n",
    "                               bnb_4bit_quant_type=quant_type_bnb,\n",
    "                               bnb_4bit_use_double_quant=double_quant_flag,\n",
    "                               bnb_4bit_compute_dtype=dtype_computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a70584648654146b9e6e85097a196d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(model_identifier, quantization_config=bnb_setup, device_map=device_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapters_path = 'D:/SUPER DATA SCIENCE/ARTIFICIAL INTELLIGENCE/phi-2 adapter'\n",
    "model = PeftModel.from_pretrained(llama_model, adapters_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained(model_identifier, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopAtPunctuation(StoppingCriteria):\n",
    "    def __init__(self, stop_token_ids):\n",
    "        self.stop_token_ids = stop_token_ids\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        last_token_id = input_ids[0, -1].item()\n",
    "        return last_token_id in self.stop_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_length=1000):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    stop_tokens = [tokenizer.encode(\"#\")[0], tokenizer.encode(\"!\")[0], tokenizer.encode(\"?\")[0]]\n",
    "    stop_criteria = StoppingCriteriaList([StopAtPunctuation(stop_tokens)])\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_length,max_new_tokens=200, pad_token_id=tokenizer.eos_token_id,stopping_criteria=stop_criteria )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    user_prompt = request.json.get('prompt')\n",
    "    generated_text = generate_text(f\"{user_prompt}\", model, llama_tokenizer)\n",
    "    return jsonify({'generated_text': generated_text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_flask():\n",
    "    app.run(host='0.0.0.0', port=5000, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.29.193:5000\n",
      "Press CTRL+C to quit\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "c:\\ProgramData\\miniconda3\\envs\\llmenv\\lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:627: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "192.168.29.13 - - [06/Sep/2024 15:13:02] \"POST /predict HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.13 - - [06/Sep/2024 15:13:38] \"POST /predict HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.13 - - [06/Sep/2024 15:14:22] \"POST /predict HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.13 - - [06/Sep/2024 15:15:02] \"POST /predict HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.13 - - [06/Sep/2024 15:15:39] \"POST /predict HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.13 - - [06/Sep/2024 15:16:08] \"POST /predict HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.13 - - [06/Sep/2024 15:17:47] \"POST /predict HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.13 - - [06/Sep/2024 15:19:16] \"POST /predict HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.13 - - [06/Sep/2024 15:19:47] \"POST /predict HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.13 - - [06/Sep/2024 15:20:12] \"POST /predict HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.13 - - [06/Sep/2024 15:21:04] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "flask_thread = threading.Thread(target=run_flask)\n",
    "flask_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flask_thread.join(timeout=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('llmenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9785629f11160b6accd1efd06c698713647ee4bed4fd28fa508b0e3b9e652ac5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
